---
layout: post
title: choosing an atom
date: 2026-01-03
---

I am quite interested in bio-inspired ML. I can't help but find it bizarre that we took inspiration from biological brains early on in ML with the Perceptron, and subsequently branched off into computer science theory for further advances. Aside from that they are massively parallel computers and trained stochastically/empirically, ML models and biological brains have very little in common. Neuroscience and ML occasionally cross paths, but it would seem we cross-pollinate ideas more from information science and mathematics more than from neuroscience or biology. Admittedly, there are very good reasons for things being the way they are.

I do hold that the brain has evolved over 80 billion years and we are only beginning to understand the surface of it's machinations today. I think the reason we turn to more exacting fields to make gains is - well, for one, it works and has good gradients of returns - but it's also an area we have more analytical understanding, whereas biology is mostly empirical. We can implement with more surety an optimisation to attention from mathematics, than a more detailed abstraction of a biological neuron.

I find an entertaining position to hold is to be generally sceptical of established ML approaches and philosophy and instead assume that nature has it right and anything that is more naturally reminiscent is probably better. I'd like to think a scenario wherein we successfully design a general intelligence is lower than a scenario wherein we facilitate the means to *discover* a general intelligence.

Optimisation and open-ended approaches are kinda similar. I think open-ended approaches (does this include RL as an example or is it just GA stuff??) simply have much higher-dimensional reward signal, which makes them less prone to inaccuracies in objective definition(?). Does this suggest the RL part of the Pretraining+RLHF pattern is where the bulk of the value lies? I guess pretraining is kinda like gene evolution - the "million year scale learning" stuff wherein you end up with valid brain geometry, and the RLHF is kinda like the plasticity of the phenotype stuff?

So, if not optimisation, backprop and designed topologies, what?

I have recently been dabbling in indirect encoding. Indirect encoding means specifying the weights of a network with some sort of genome. So, rather than optimising weights *directly* (with backprop), you optimise a set of rules that generate a network. There are advantages and disadvantages to this approach.

Some examples:

Compressed Network Search:
Similar to images, neural networks can also be represented as a sum of signals. We use this in JPEG compression to reduce file sizes, but we can also use this technique to reduce the search space of a neural network (although in this case we don't throw out high-frequency information). Rather than mutating weights themselves, we can make thematic changes across a whole network by just tweaking one parameter.

CPPNs:
Coordinate Pattern Producing Networks are themselves neural nets. They can be used as hypernetworks - networks that create networks. In the most basic case, they take coordinates of a presynaptic (x_pre, y_pre) and postsynaptic neuron (x_post, y_post) and from that "paint" a weight into a target network. The thinking behind this is that the CPPN encodes geometry of the brain into it's own weights. As an example, a CPPN might learn to connect neurons nearer a retina input to an area of the brain associated with image processing, and that would result in the target network having good performance. These CPPNs are usually quite small and have more varied topology compared to typical neural networks. When used as an indirect encoding, these are usually developed using an algorithm called HyperNEAT.

Some more options include:
- l-systems - fractal-like grammars
- cellular encoding


Environment Richness:
Pretrain/RLHF just represents a really good simulation of a language environment. 

Selecting the atom:
If we wanted to model a simple digital brain with recurrence and plasticity, optimisation-based approaches become quite difficult.

Genetic algorithms and indirect encoding become quite attractive options. However, there are some very difficult choices before we get started:
- We need to select a **Neuron Model**. Neurons are incredibly complex systems, and abstracting them in software can be done at varying levels of detail:
    - We can simply model activations, as in the perceptron and canonical deep learning models
    - We can model them as integrators over time - for example, Leaky Integrate and Fire neurons
    - We can model them as resonators - Resonate-and-Fire neurons
    - We can assume we'll need more detail, and go for the izhikevich approach, characterising neurons with four parameters
    - We can simulate a neuron in detail via a very complex model - Hodgkin-Huxley 
    - We can approximate a neuron using an MLP, a la Continuous Thought Machines (I have slight concerns about the fixed memory length of these neurons)

- We need to define a **Fitness Function**.
    - Behavioural novelty (commonly: Novelty Search) is an attractive option as it provides selection pressure with minimal bias from the practitioner
    - Quality of some delineation: How many resources the agent gets, how tall it is - are these 

- We need to detail a **Genome and Encoding** method that we hope is capable of producing a valid network
    - if we use a Compressed Network Search approach but, say, filter out high frequencies, it may never be able to produce a brain with valid or stable circuitry
    - if we use a CPPN but only provide it coordinate inputs, and don't provide basis signals(?) (like sin(x) or sin(y)), it may never approximate patterns

- We need to select an **Evolutionary Strategy** that facilitates healthy growth. There are many things in the GA toolbox - speciation, for example, protects phenotypes that are similar in an effort to let them mutate out of any discontinuities in the fitness landscape. For example, if land creatures evolve with feathers, speciation will let those creatures compete against only feathered creatures, possibly until those creatures develop the ability to fly. However if we had them competing against the entire kingdom, they would always get beaten out by elephants.




<!-- 
***L-system GAs?***
could we have genetic algorithms with fractal speciation?
branch length from root is the fitness function?
quantity of children is the fitness function?

Is the trend towards increasing abstraction in computing simply because reducing inductive biases produces more effective systems? Inductive biases constitute fragility? -->


<!-- should i just do a writeup on gifbreeder...? -->