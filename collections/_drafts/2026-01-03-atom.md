---
layout: post
title: choosing an atom
date: 2026-01-03
---

I am quite interested in bio-inspired ML. I can't help but find it bizarre that we took inspiration from biological brains early on in ML with the Perceptron, and subsequently branched off into computer science theory for further advances. Aside from that they are massively parallel computers and trained stochastically/empirically, ML models and biological brains have very little in common. Neuroscience and ML occasionally cross paths, but it would seem we cross-pollinate ideas more from information science and mathematics more than from neuroscience or biology. Admittedly, there are very good reasons for things being the way they are.

I do hold that the brain has evolved over 80 billion years and we are only beginning to understand the surface of it's machinations today. I think the reason we turn to more exacting fields to make gains is - well, for one, it works and has good gradients of returns - but it's also an area we have more analytical understanding, whereas biology is mostly empirical. We can implement with more surety an optimisation to attention from mathematics, than a more detailed abstraction of a biological neuron.

I find an entertaining position to hold is to be generally sceptical of established ML approaches and philosophy and instead assume that nature has it right and anything that is more naturally reminiscent is probably better. I'd like to think a scenario wherein we successfully design a general intelligence is lower than a scenario wherein we facilitate the means to *discover* a general intelligence.

Optimisation and open-ended approaches are kinda similar. I think open-ended approaches (does this include RL as an example or is it just GA stuff??) simply have much higher-dimensional reward signal, which makes them less prone to inaccuracies in objective definition(?). Does this suggest the RL part of the Pretraining+RLHF pattern is where the bulk of the value lies? I guess pretraining is kinda like gene evolution - the "million year scale learning" stuff wherein you end up with valid brain geometry, and the RLHF is kinda like the plasticity of the phenotype stuff?

So, if not optimisation, backprop and designed topologies, what?

I have recently been dabbling in indirect encoding. Indirect encoding means specifying the weights of a network with some sort of genome. So, rather than optimising weights *directly* (with backprop), you optimise a set of rules that generate a network. There are advantages and disadvantages to this approach.

Some examples:

Compressed Network Search:
Similar to images, neural networks can also be represented as a sum of signals. We use this in JPEG compression to reduce file sizes, but we can also use this technique to reduce the search space of a neural network. Rather than mutating weights themselves, we can mutate the constituent frequencies in the weight matrix, which allows us to make thematic changes to a network by mutating or adjusting individual frequency parameters.

CPPNs:
Coordinate Pattern Producing Networks are themselves neural nets. In the most basic case, they take coordinates of a presynaptic (x_pre, y_pre) and postsynaptic neuron (x_post, y_post) and from that "paint" a weight into a target network. The thinking behind this is that the CPPN encodes geometry of the brain into it's own weights. As an example, a CPPN might learn to connect neurons nearer a retina input to an area of the brain associated with image processing, and that would result in the target network having good performance. These CPPNs are usually quite small and have more varied activations than normal neural networks.

Environment Richness:
Pretrain/RLHF just represents a really good simulation of a language environment. 